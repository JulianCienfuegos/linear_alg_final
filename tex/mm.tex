\chapter{Dense Matrix Multiplication}
\section{C++}
\subsection{Code}
\begin{center}
\textbf{This is the code we will be discussing in this section.}
\begin{lstlisting}
/*
 * This code surveys matrix multiplication in C++
 * 
 * 
 * BLAS
 * Tiled MM on one core
 * Tiled MM on multiple cores
 * Armadillo MM
 * Eigen MM
 * 
 * are what we will experiment with.
 * 
 * Compilation Instructions: 
 * g++ -I /usr/local/include/eigen3 CompareMM.cpp -larmadillo -lblas -o MM
 * 
 * Where -I /usr/local/include/eigen3 is the path to your Eigen directory.
 * 
 */  

#include <stdio.h>
#include <ctime>
#include <armadillo>
#include <omp.h>
#include <Eigen/Dense>

#define bSize 64
#define NITER 1
int N = 1024;

/*
 * The function dgemm() performs one multiplication of the form 
 * C = alpha*A*B + beta*C. Below we will set alpha to 1 and beta to zero.
 */

extern ``C" void dgemm_(char *transa, char *transb, int *m, int *n, int* k, 
			 double *alpha, double *A, int *lda, double *B, int *ldb,
			 double *beta,  double *C, int *ldc ); 


void BMM(double **, double **, double **);
void omp_BMM(double **, double **, double **);

using namespace std;
using namespace arma;
using Eigen::MatrixXd;

int main(int argc, char * argv[])
{
/************************ MAKE MATRICES **************************/
/*                         C++ 2D ARRAY                          */
double ** A_block = new double * [N];
double ** B_block = new double * [N];
double ** C_block = new double * [N];

for(int i = 0; i < N; i++)
{
	A_block[i] = new double [N];
	B_block[i] = new double [N];
	C_block[i] = new double [N];
}
srand(time(NULL));
for(int i = 0; i < N; i++)
	for (int j = 0; j < N; j++)
	{
		A_block[i][j] = (double)(rand()%100);
		B_block[i][j] = (double)(rand()%100);
	}	
	
/*                        C++ 1D ARRAY                           */
double * A_blas = new double [N*N];
double * B_blas = new double [N*N];
double * C_blas = new double [N*N];
for (int i = 0; i < N*N; i++)
{
	A_blas[i] = (double)(rand()%100);
	B_blas[i] = (double)(rand()%100);
}

/*                         ARMADILLO MATRICES                    */
mat A = randu<mat>(N, N);
mat B = randu<mat>(N, N);
mat C = zeros(N,N);

/*                          EIGEN MATRICES                       */

MatrixXd A_eigen = MatrixXd::Random(N,N);
MatrixXd B_eigen = MatrixXd::Random(N,N);
MatrixXd C_eigen = MatrixXd::Zero(N,N);

/*                         OTHER VARIABLES                       */
double run_times[5];
clock_t start, stop;
char trans = 't';
double alpha = 1;
double beta = 0;
/*                           BEGIN COMPARE!                      */

printf("Running BMM...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	BMM(A_block, B_block, C_block);
stop = clock();
run_times[0] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running omp_BMM...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	omp_BMM(A_block, B_block, C_block);
stop = clock();
run_times[1] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running Armadillo MM...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	C = A*B;
stop = clock();
run_times[2] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running BLAS...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	dgemm_(&trans, &trans, &N, &N, &N, &alpha, A_blas, &N, B_blas,\
		&N, &beta, C_blas, &N);
stop = clock();
run_times[3] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running Eigen...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	C_eigen = A_eigen*B_eigen;
stop = clock();
run_times[4] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);
	
printf("BMM: %f\nomp_BMM: %f \nArmadillo: %f\nBLAS: %f\nEigen: %f\n", \
	run_times[0], run_times[1], run_times[2], run_times[3], run_times[4]);

}

void BMM(double ** A, double ** B, double ** C)
{
/* Block Matrix Multiply */
for(int i = 0; i < N; i += bSize)
	for(int k = 0; k < N; k += bSize)
		for(int j = 0; j < N; j += bSize)
			for(int it = i; it < i+bSize; it++)
				for(int kt = k; kt < k+bSize; kt++)
					for(int jt = j; jt < j+bSize; jt++)
					{
						C[it][jt] += A[it][kt]*B[kt][jt];
					}
}


void omp_BMM(double ** A, double ** B, double ** C)
{
int i, j, k, it, jt, kt;
/* openMP version of BMM */
#pragma omp parallel shared(A,B,C), private(i,j,k, it, jt, kt)
{
#pragma omp for schedule(static)
for(int i = 0; i < N; i += bSize)
	for(int k = 0; k < N; k += bSize)
		for(int j = 0; j < N; j += bSize)
			for(int it = i; it < i+bSize; it++)
				for(int kt = k; kt < k+bSize; kt++)
					for(int jt = j; jt < j+bSize; jt++)
						C[it][jt] += A[it][kt]*B[kt][jt];
}
}

\end{lstlisting}
\textbf{Results}
\begin{lstlisting}
(With -O3 optimization)
BMM: 1.654067
omp_BMM: 1.621354 
Armadillo: 1.439744
BLAS: 1.430144
Eigen: 0.644094
(With no compiler optimization)
BMM: 7.912810
omp_BMM: 7.815276 
Armadillo: 1.446131
BLAS: 1.428944
Eigen: 24.925716

\end{lstlisting}
\end{center}

\subsection{Tiled Matrix multiplication}
This is an implementation of matrix multiplication written in pure C++, which uses tiling to achieve better temporal locality of data. The computer has a fast cache memory closer to the CPU registers than the main memory, and every time a bit of main memory is accessed, the computer brings a whole chunk of adjacent memory to the cache. What tiling achieves is to break the matrix multplication up into multiplications of sub matrices by sub-matrices, such that the chunks which the computer caches are not wasted, but are used over and over again until tey are not needed. For example, consider the multiplication AB, where A and B are NxN matrices. The element in the upper left corner of A will eventually be multiplied by every element in the first row of B. What tiling acheieves is that the indicated element of A is multiplied by many elements of B in a very short period of time, so that this element does not need to be fetched from main memory N times, once for each of the N elements in the first row of B.

My implementation is fairly straight forward. I allocate some square 2d arrays and then multiply them using the traditional matrix multiplication for loops, except that the three inner for loops work only on a subset of data specified by the outer for loops.

\subsection{Parallelized MM using OpenMP}
OpenMP is the easiest library to use to parallelize C++ code. All one needs to do is include the headerfile omp.h, add one or two lines of code before a for loop, and then the code will run on all of the cores of a multicore processor. Unfortunatly, in the above example, little performance increase is seen. This is likely due to "False Sharing", which is what occurs when multiple processors concurrently try to write to adjacent locations in global memory. Since each processor has its own cache, and memory is brought to the cache in chunks, if processor 1 is working on A[0] and processor 2 is writing to A[1], no parallelization is possible. Let me explain more.
	Processor 1 will write its work to A[0], and send the chunk A[0]...A[j] back to global memory, where A[0] ... A[j] is the chunk of A fetched by the cache. Processor 2 will then be unable to send its results back to global memory since processor 2 also fetched A[0]...A[j]. and the value of A[0] which it has does not match the global value. Processor 2 thus checks out the same memory block, redoes its computation, and then sends this value back to memory. Only now have both A[0] and A[1] been updated. The code can thus run slower than the non parallelized code, due to the excess memory transfers. This can be alleviated with relatively simple loop transformations, but I haven't spent time on that. I chose to simply present OpenMP as an option which the interested reader can pursue. 
	If we were to write a clever memory access pattern for the matrix multiplication, there would be no competition from the other libraries, because they all run on one core. This is a major drawback to BLAS, and can be attributed to its having been written a long time ago when multicore architectures were not available. There are some parallel BLAS implementations available online, and even some which work with CUDA, the C-based GPU programming language. These parallel BLAS implementations boast impressive performance but are unfortunately outside of the scope of this paper.

\subsection{BLAS}
The BLAS are a set of old FORTRAN routines that have been highly optimized over time. In addition to being fast, they are also portable and can be run on many processors. One interesting aspect of the BLAS is the fact that they may give <seemingly> erroneous results if used in a C++ code. This is because FORTRAN uses column major storage of matrices, whereas C++ uses row major storage. You will notice in the sample code that dgemm takes a pointer to a 1d array as an arguement. Depending on how you want to output the results, you should enter the entries of your matrices carefully. If you enter them in a row major fashion, your output will be transposed. If you decide to circumvent this by using the TRANS argument to dgemm, you have to be careful with the values of LDX, which are described below.

The arguments to the mysteriously named dgemm are:
\begin{enumerate}
\item TRANSX - This is whether or not you want the matrix X transposed. Choices are 'N' for No, 'T' for Transpose, and 'C' for Conjugate Transpose.
\item M - This specifies the number of rows in A and C
\item  N - This specifies the number of columns in B and A
\item K - This specifies the number of columns of A and, consequently, the rows of B.
\item LDX - This specifies the leading dimension of matrix X. This depends upon whether or not you use transposition. For example, if TRANSA = 'T', then LDA=N. If TRANSA='N', then LDA=M.
\item ALPHA, BETA - These are coefficients which correspond the the operation performed by dgemm(), namely, $C = \alpha A*B + \beta C$. In the sample code, $\beta$ is set to zero so we simply perform a matrix multiplication.
\item A, B, C - These are the matrices we are using. These must be matrices of double precision floats, stored as a 1D array.
\end{enumerate}
\subsection{Armadillo}
Armadillo is a widely used linear algebra package which is largely built upon the BLAS and LAPACK, and which offers a convenient MATLAB-like syntax to the user. You will notice in the above code that sprawling lines of code are needed for the tiled matrix multiplication, and that the confusing syntax which is required to implement dgemm makes it frightening to the enduser. All of this complication boils down to four easy to read lines when the matrix product is coded using Armadillo. Operators are nicely overloaded in Armadillo, too, such that if one wants to see the contents of matrix A one can use the familiar cout << A << endl, and the output is nicely formatted. In addition, matrix multiplication uses *, which makes it easy to code (even the super user-friendly language Python 'requires' users to use dot(A,B) to compute  the product A*B).

Since Armadillo is essentially a front end to the BLAS, we should see very little performance difference between it and the pure BLAS code. Indeed, the overhead generated by Armadillo is quite small, and the run times are about the same for both.

\subsection{Eigen}
The Eigen syntax is not terribly complicated, but is more confusing than that of Armadillo. In addition, downloading and linking Eigen is slightly more complicated in that you have to include a header file and provide a symlink to the directory where certain functions are stored when you compile. For the user not thoroughy comfortable with a UNIX environment, this is a serious hassle. The good news is, however, that Eigen provides not only common linear algebra operations, but also comes with a whole suite of overloaded operators which make using the library a real pleasure. These are the reasons why Eigen is used by so many production quality software development teams. 
The performance if Eigen is very mysterious, however. It seems that Eigen requires a great deal of overhead which O3 optimization miraculously gets rid of - notice that the run times with and without optimization are two orders of magnitude apart! While Eigen is easy to use, the fact that the run time with and without optimizatin is so disparate make one wonder what exactly the compiler is throwing away and if it is safe. O3 optimized Eigen code runs much faster than optimized BLAS code, and seeing how Eigen is based on BLAS, one ought to feel some unease with the shortcuts taken by the compiler. In short, Eigen may not be trustworthy.
 
\section{Python}
\subsection{Code}
\begin{center}
\textbf{Here is the code we will be discussing.}
\begin{lstlisting}
""" In this script we will see how to compute matrix - matrix products
using numpy and scipy.linalg.BLAS 

From the output we see the python methods are similar.
"""

from numpy import matrix, array, dot, zeros
from numpy.random import random
from scipy.linalg.blas import dgemm
import time

N = 1024
NITER = 30

A_array = random((N,N))
B_array = random((N,N))
C_array = zeros((N,N))

start = time.time()
for it in range(NITER):
	C_array = dot(A_array,B_array)
stop = time.time()

print"Time for 2d array dot product is:", (stop - start)/NITER

A_mat = matrix(A_array)
B_mat = matrix(B_array)
C_mat = matrix(C_array)

start = time.time()
for it in range(NITER):
	C_mat = A_mat*B_mat
stop = time.time()
print "Time for Matrix multiplication is:", (stop-start)/NITER

alpha = 1
start = time.time()
for it in range(NITER):
	C_array = dgemm(alpha, A_array, B_array)
stop = time.time()
print "Time for dgemm is:", (stop-start)/NITER
\end{lstlisting}
\textbf{Results}
\begin{lstlisting}
Time for 2d array dot product is: 0.299135661125
Time for Matrix multiplication is: 0.304835136731
Time for dgemm is: 0.367184797923
\end{lstlisting}
\end{center}
\subsection {A quick comparison}
It is easy to talk about Python code, because it is so succinct. In this code I compare matrix multiplication using numpy 2d arrays, numpy 2d matrices, and the low-level BLAS function dgemm. One right away notices that the array and matrix multiplications run in exactly the same amount of time. This is because numpy matrices are nothing more than numpy arrays with a bit more structure. One can quickly see this since the matrices above are instantiated by copying arrays. Indeed, the Numpy source code shows that Matrices and Arrays are fundamentally the same object. Why do both exist, then?
I am not sure why Numpy includes a Matrix class. The numpy website actually recommends that users stay away from matrices as they are not as heavily tested as the matrices are. In fact, the numpy website offers a quick answer the the question ``Should I use arrays or matrices?" ``Short Answer: Use Arrays".\footnote{\url{http://wiki.scipy.org/NumPy_for_Matlab_Users}} In  the above code one can see that the matrix multiplication is done with arrays using the dot() function, and with matrices using the convenient *. Since matrix multiplication is so much more intuitive with the matrix class, one might be tempted to use matrix all the time instead of the array class. Do so at your own risk, because the Numpy developers say not to use the use matrices, even though they created matrices and put them within the reach of the user. See figure 1.
\begin{figure}
\centering
\includegraphics[scale = 0.5]{AE.jpeg}
\caption{Look But Don't Touch}
\end{figure} 

You can also see that dgemm is a bit slower than the built in Python functions. This is rather strange due the  fact that the numpy routines are built upon BLAS and LAPACK. This, just like the Eigen example, needs much further investigation.
The really interesting bit of information is that the \emph{Python code runs much faster than the C++ code} in either case! This is exactly the opposite of our intuition and dispells the common stereotype that Python is slow and useless for scientific computing. In these examples, the Python optimizer is able to far outdo the g++ compiler! (Unfortunaely there is no way to link Eigen and Armadillo to the icc compiler, so I was unable to see how fast my code would be in that case. Typically, ICC far outdoes g++).