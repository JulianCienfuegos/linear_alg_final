\chapter{LU Factorization}
\section{C++}
\subsection{Code}
\begin{center}
\textbf{Here is the code we will be using}
\begin{lstlisting}
/* 
 * This code surveys C++ linear system solvers which use the LU
 * Factorization. We will consider:
 * 
 * BLAS
 * An LU factorization and solver which I wrote.
 * Armadillo
 * Eigen
 * 
 * Compile with: g++ -I /usr/local/include/eigen3 CompareLU.cc \
 *            -larmadillo -llapack -O3 -o CompareLU
 */

#include <iostream>
#include <cmath>
#include <ctime>
#include <cstdlib>
#include <armadillo>
#include <Eigen/Dense>

int N = 1024;
const int num_iters = 10;

extern "C" void dgetrf_(int* M, int* N, double* a, int* lda, \
	int* ipiv, int* info);
extern "C" void dgetrs_(char *TRANS, int *N, int *NRHS, double *A, \
	int *LDA, int *IPIV, double *B, int *LDB, int *INFO );

using namespace std;
using namespace arma;
using Eigen::MatrixXd;
using Eigen::PartialPivLU;

void lufac(double **, double **);
void fSub(double **, double *, double *);
void bSub(double **, double *, double *);
void MVMult(double **, double *, double *);

int main()
{
	/********************** ALLOCATE MEMORY ***************************/
	/*                   ARRAYS FOR MY SOLVER                         */
	double ** A = new double * [N];
	double ** P = new double * [N];
	double ** L = new double * [N];
	double ** U = new double * [N];
	double * b = new double [N];
	double * b_hat = new double [N];
	double * y = new double [N];
	double *x = new double [N];

	for(int i = 0; i < N; i++)
	{
		A[i] = new double [N];
		P[i] = new double [N];
		L[i] = new double [N];
		U[i] = new double [N];
	}
	
	srand((signed)time(NULL));
	
	for (int i = 0; i < N; i++)
	{
	  for (int j = 0; j < N; j++)
	  {
		  A[i][j] = rand() % 100 + 1;
		  L[i][j] = 0;
		  U[i][j] = 0;
		  if(i == j) // Make the identity matrix
			P[i][j] = 1;
		  else
			P[i][j] = 0;
	  }
	}

	for (int i = 0; i < N; i++)
	{
		b[i] = rand() % 100 + 1;
	}
	/*                    BLAS SOLVER ARRAYS                          */
	double * A_blas = new double [N*N];
	double * b_blas = new double [N];
	int ipiv[N];
	for (int i = 0; i < N*N; i++)
		A_blas[i] = (double)(rand()%100);
	for (int i = 0; i < N; i++)
		b_blas[i] = (double)(rand()%100);
	/*                  ARMADILLO MATRICES                           */
	mat A_arma = randu<mat>(N, N);
	mat P_arma, L_arma, U_arma;
	mat b_arma = zeros<mat>(N,1);
	mat x_arma = zeros<mat>(N,1);
	
	/*                    EIGEN MATRICES                             */
	MatrixXd A_eigen = MatrixXd::Random(N,N);
	MatrixXd x_eigen = MatrixXd::Zero(N,1);
	MatrixXd b_eigen = MatrixXd::Random(N,1);
	/*                    OTHER VARIABLES                            */
	char trans = 'N';
	int dim = N;
	int nrhs = 1;
	int LDA = dim;
	int LDB = dim;
	int info;
	double run_times[4];
	clock_t start, stop;

	/************************** MY SOLVER *****************************/
	printf("Running My Solver...\n");
	start = clock();
	for(int iter = 0; iter < num_iters; iter++)
	{
		lufac(A,P);
	
		/* Get U */
		for (int i = 0; i < N; i++)
		{
			for(int j = i; j < N; j++)
			{
				U[i][j] = A[i][j];
			}
		}
	
		/* Get L */
		for(int i = 0; i < N; i++)
		{
			L[i][i] = 1;
			for(int j = i-1; j >=0; j--)
			{
				L[i][j] = A[i][j];
			}
		}
			
		/*Now we multiply b by P and solve PAx = Pb => LUx = Pb*/
		MVMult(P, b, b_hat);
	
		/* Solve Ly = b_hat */
		fSub(L, y, b_hat);
	
		/* Solve Ux =  y */
		bSub(U, x, y);
		
		/* Now we can do what we want with the x! */
	}
	stop = clock();
	run_times[0] = (double)(stop - start)/(num_iters*CLOCKS_PER_SEC);
	/* We can clean up a bit */
	for (int i = 0; i < N; i++)
	{
		delete[] A[i];
		delete[] P[i];
		delete[] L[i];
		delete[] U[i];
	}
	delete[] A;
	delete[] P;
	delete[] L;
	delete[] U;
	delete[] b;
	delete[] b_hat;
	delete[] x;
	delete[] y;
	
	/*********************** BLAS SOLVER ******************************/
	printf("Running BLAS...\n");
	start = clock();
	for(int iter = 0; iter < num_iters; iter++)
	{
		for (int i = 0; i < N; i++)
			b_blas[i] = (double)(rand()%100);
		dgetrf_(&dim, &dim, A_blas, &LDA, ipiv, &info);
		dgetrs_(&trans, &dim, &nrhs, A_blas, &LDA, \
			ipiv, b_blas, &LDB, &info);
	}
	stop = clock();
	clock_t start1 = clock();
	for(int i = 0; i < num_iters; i++)
	{
		for (int i = 0; i < N; i++)
			b_blas[i] = (double)(rand()%100);
	}
	clock_t stop1 = clock();
	run_times[1] = (double)(stop - start)/(num_iters*CLOCKS_PER_SEC);
	run_times[1] -= (double)(stop1 - start1)/(num_iters*CLOCKS_PER_SEC);
	delete[] A_blas;
	delete[] b_blas;
	
	/************************ ARMADILLO *******************************/
	printf("Running Armadillo...\n");
	start = clock();
	for(int iter = 0; iter < num_iters; iter++)
	{
		lu(L_arma, U_arma, P_arma, A_arma);
		x_arma = solve(trimatu(U_arma), solve(trimatl(L_arma), \
			P_arma*b_arma));	
	}
	stop = clock();
	run_times[2] = (double)(stop - start)/(num_iters*CLOCKS_PER_SEC);
	
	/************************** EIGEN *********************************/
	printf("Running Eigen...\n");
	
	start = clock();
	for(int iter = 0; iter < num_iters; iter++)
	{
		x_eigen = A_eigen.lu().solve(b_eigen);
	}
	stop = clock();
	run_times[3] = (double)(stop - start)/(num_iters*CLOCKS_PER_SEC);
	
	/********************* PRINT RESULTS ******************************/
	printf("My Solver: %f\nBLAS: %f \nArmadillo: %f\nEigen: %f\n", \
		run_times[0], run_times[1], run_times[2], run_times[3]);
}

void lufac(double ** A, double ** P)
{
	for(int i = 0; i < N-1; i++) // loop over all columns except the last.
	{
		
		double max = abs(A[i][i]);
		int maxIdx = i; //row i has the max right now
		for(int j = i+1; j < N; j++) //loop over rows
		{
			if(abs(A[j][i]) > max){maxIdx = j;} // Find max entry
		}
		if(maxIdx != i) // Perform a swap if the max isn't on the diagonal.
		{
			for(int col = 0; col < N; col++) //swap rows of A and P
			{
				double temp = A[i][col];
				A[i][col] = A[maxIdx][col];
				A[maxIdx][col] = temp;
				double temp2 = P[i][col];
				P[i][col] = P[maxIdx][col];
				P[maxIdx][col] = temp2;
			}
		}
		
		for (int j = i+1; j < N; j++)
		{
			A[j][i] = A[j][i]/A[i][i];
		}
		for(int j = i+1; j < N; j++)
		{
			for (int k = i+1; k < N; k++)
			{
				A[j][k] = A[j][k] - A[j][i]*A[i][k];
			}
		}
	}
}

void bSub(double ** U, double * x, double * b)
{
	for(int i = N-1; i >=0 ; i--)
	{
		for(int j = i+1; j < N; j++)
		{
			x[i] -= x[j]*U[i][j]; 
		}
		x[i] = (x[i] + b[i])/U[i][i];
	}

}

void fSub(double ** L, double * x, double * b)
{
	for(int i = 0; i < N; i++)
	{
		for (int j = 1; j < i; j++)
		{
			b[i] -= L[i][j]*x[j];
		}
		x[i] = b[i]/L[i][i];
	}
}

void MVMult(double ** M, double * x, double * b)
{
	for(int i = 0; i < N; i++)
	{
		b[i] = 0; /* This could be done elsehwere*/
	}
	/*Could this be optimized?*/
	for(int i = 0; i < N; i++)
	{
		for(int j = 0; j < N; j++)
		{
			b[i] += M[i][j]*x[j];
		}
	}
}

\end{lstlisting}
\textbf{Results}
\begin{lstlisting}
(With O3 optimization, time in seconds)
My Solver: 0.653896
BLAS: 0.427258 
Armadillo: 0.447107
Eigen: 0.285615
(Without O3 optimization)
My Solver: 2.659308
BLAS: 0.430189 
Armadillo: 0.456686
Eigen: 9.694578
\end{lstlisting}
\end{center}
\subsection{My LU solver}
My solver is clunky, large and unoptimized. We should not expect great performance, and indeed we do not see great performance. Without optimization, however, this code outperforms Eigen! An interesting project would be to read the Eigen source code, because its performance with the g++ compiler is quite mysterious. 
\subsection{BLAS}
In order to solve a linear system using the BLAS, one has to use two functions. The first one is dgetrf , which factors the matrix A, and the next one is dgetrs, which solves the system given a right hand side $\vec{b}$. dgetrf factors A using partial pivoting and overwrites LU onto A. It also outputs an array IPIV, which is the pivot matrix P.
Here are the inputs for dgetrf:
\begin{enumerate}
\item M - The number of rows of A
\item N - The number of columns of A
\item A - An array of doubles. Note: This routine does not work for floats.
\item LDA - Leading dimension of A. This seems a bit redundant, as M is the leading dimension of A. This parameter may be necessary for some reason which is not yet clear to me.
\item INFO - The function stores a zero in INFO for successful completion, or an error code otherwise.
\end{enumerate}
And here are the inputs for dgetrs:
\begin{enumerate}
\item TRANSX - The same as for dgemm
\item N - The order of the matrix A. A is a square matrix.
\item NHRS - Number of Right Hand Sides. This solver can solve multiple systems given multiple b's
\item A, b - The matrices in Ax = b. b does not have to be a column. b can store multiple right hand sides.
\item LDX - Leading dimension of matrix X.
\item  IPIV - the pivot matrix for A.
\item INFO - The same as above.
\end{enumerate}

\subsection{Armadillo}
The Armadillo library does not have an LU solver, but it does have a solver routine which uses an unspecified decomposition to solve the linear system. Therefore, I had to use a bit of trick to solve the system. I used the Armadillo lufactor routine, and then use the solve() routine to solve the corresponding triangular systems. Below you can see that the lu routine takes in three blank arrays to store the L U and P arrays, and the array A to be factored. Then, on the next line you can see how I chained together two solve()s in order to solve the triangular systems. Armadillo does not over write A and in this way loses a bit of performance due to memory transfers, but even for a square matrix of order 1024, the effect of the extra matrices is negligible when one compares this routine to the performance of the pure BLAS routine.
\begin{lstlisting}
lu(L_arma, U_arma, P_arma, A_arma);
x_arma = solve(trimatu(U_arma), solve(trimatl(L_arma), P_arma*b_arma));	
\end{lstlisting}
Once again, Armadillo offers a clear, concise syntax and relatively fast code.

\subsection{Eigen}
I had some brief difficulty using the LU solver from Eigen. I had to dig around online forums in order to find out how to factorize the matrix and then again to figure out how to solve the system. It turns out that the two functions can be chained together, as is seen in the above code. In the end, this makes for very clean code. 
The next difficulty I came across was deciding what part of the Eigen namespace to use. The partial pivoting routine is called lu(), but in order to use this routine one needs the line \textbf{using Eigen::PartialPivLU}, which is somewhat unituitive. I found the Armadillo documentation much easier to navigate, and had no issue when importing specific Armadillo functions. 

As we saw with the Eigen matrix multiplier, the lu solver is very slow without optimization, and then inexplicably fast when compiler optimization is used. I would like to read the Eigen source code to see why this happens. It could be that Eigen uses some form of parallelism, or that it has some other trick.

\section{Python}
\subsection{Code}
\begin{center}
\textbf{Here is the code we are considering}
\begin{lstlisting}
""" Remember the Python zen: There should be only one obvious way to do 
things. We are going to compare the low-level LAPACK functions 
degtrf and dgetrs to the built in scipy LU routines. """

from numpy import array
from scipy.linalg import lu_solve, lu_factor
from scipy.linalg.lapack import dgetrf, dgetrs
from numpy.random import random
import time

# Make arrays
NUM_ITER = 10
N = 1024
A = random((N,N))
b = random((N,1)) 

# Solve using scipy.linalg
start = time.time()
for it in range(NUM_ITER):
	(LU_and_piv) = lu_factor(A)
	(x) = lu_solve(LU_and_piv, b)
stop = time.time()
print "Time for scipy routine is", (stop - start)/NUM_ITER

# Solve using scipy.linalg.lapack
start = time.time()
for it in range(NUM_ITER):
	(LU, piv, info) = dgetrf(A)
	(x) = dgetrs(LU, piv, b)
stop = time.time()
print "Time for LAPACK routine is", (stop - start)/NUM_ITER
\end{lstlisting}
\textbf{Results}
\begin{lstlisting}
Time for scipy routine is 0.147462892532
Time for LAPACK routine is 0.144656896591
\end{lstlisting}
\end{center}
\subsection{A brief discussion of the Results}
True to the python philosophy, there is really only one way to solve a problem using LU factorization in Python. Much of scipy is built upon the BLAS and LAPACK, and it turns out that the scipy lu\_solve() and lu\_factor() routines are nothing more than front ends to the LAPAK functions dgetrf and dgetrs. As you can see above, the run time is the same for the LAPACK and the Scipy routines. Indeed, one can read the Scipy source code to confirm this suspicion. This entire Scipy source code is on GitHub.
Interestingly enough, the Python codes outperform the equivalent C++ codes again. This could be due to the g++ compiler, or it could be attributable to some other reason. The deserves more experimentation on more computers, and with other compilers. This experiment would be particularly interesting to perform with the ICC compiler. 
