\chapter{Dense Matrix Multiplication}
\section{C++}
\subsection{Code}
\begin{center}
\textbf{This is the code we will be discussing in this section.}
\begin{lstlisting}
/*
 * This code surveys matrix multiplication in C++
 * 
 * 
 * BLAS
 * Tiled MM on one core
 * Tiled MM on multiple cores
 * Armadillo MM
 * Eigen MM
 * 
 * are what we will experiment with.
 * 
 * Compilation Instructions: 
 * g++ -I /usr/local/include/eigen3 CompareMM.cpp -larmadillo -lblas -o MM
 * 
 * Where -I /usr/local/include/eigen3 is the path to your Eigen directory.
 * 
 */  

#include <stdio.h>
#include <ctime>
#include <armadillo>
#include <omp.h>
#include <Eigen/Dense>

#define bSize 64
#define NITER 1
int N = 1024;

/*
 * The function dgemm() performs one multiplication of the form 
 * C = alpha*A*B + beta*C. Below we will set alpha to 1 and beta to zero.
 */

extern ``C" void dgemm_(char *transa, char *transb, int *m, int *n, int* k, 
			 double *alpha, double *A, int *lda, double *B, int *ldb,
			 double *beta,  double *C, int *ldc ); 


void BMM(double **, double **, double **);
void omp_BMM(double **, double **, double **);

using namespace std;
using namespace arma;
using Eigen::MatrixXd;

int main(int argc, char * argv[])
{
/************************ MAKE MATRICES **************************/
/*                         C++ 2D ARRAY                          */
double ** A_block = new double * [N];
double ** B_block = new double * [N];
double ** C_block = new double * [N];

for(int i = 0; i < N; i++)
{
	A_block[i] = new double [N];
	B_block[i] = new double [N];
	C_block[i] = new double [N];
}
srand(time(NULL));
for(int i = 0; i < N; i++)
	for (int j = 0; j < N; j++)
	{
		A_block[i][j] = (double)(rand()%100);
		B_block[i][j] = (double)(rand()%100);
	}	
	
/*                        C++ 1D ARRAY                           */
double * A_blas = new double [N*N];
double * B_blas = new double [N*N];
double * C_blas = new double [N*N];
for (int i = 0; i < N*N; i++)
{
	A_blas[i] = (double)(rand()%100);
	B_blas[i] = (double)(rand()%100);
}

/*                         ARMADILLO MATRICES                    */
mat A = randu<mat>(N, N);
mat B = randu<mat>(N, N);
mat C = zeros(N,N);

/*                          EIGEN MATRICES                       */

MatrixXd A_eigen = MatrixXd::Random(N,N);
MatrixXd B_eigen = MatrixXd::Random(N,N);
MatrixXd C_eigen = MatrixXd::Zero(N,N);

/*                         OTHER VARIABLES                       */
double run_times[5];
clock_t start, stop;
char trans = 't';
double alpha = 1;
double beta = 0;
/*                           BEGIN COMPARE!                      */

printf("Running BMM...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	BMM(A_block, B_block, C_block);
stop = clock();
run_times[0] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running omp_BMM...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	omp_BMM(A_block, B_block, C_block);
stop = clock();
run_times[1] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running Armadillo MM...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	C = A*B;
stop = clock();
run_times[2] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running BLAS...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	dgemm_(&trans, &trans, &N, &N, &N, &alpha, A_blas, &N, B_blas,\
		&N, &beta, C_blas, &N);
stop = clock();
run_times[3] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);

printf("Running Eigen...\n");
start = clock();
for(int i = 0; i < NITER; i++)
	C_eigen = A_eigen*B_eigen;
stop = clock();
run_times[4] = (double)(stop - start)/(NITER*CLOCKS_PER_SEC);
	
printf("BMM: %f\nomp_BMM: %f \nArmadillo: %f\nBLAS: %f\nEigen: %f\n", \
	run_times[0], run_times[1], run_times[2], run_times[3], run_times[4]);

}

void BMM(double ** A, double ** B, double ** C)
{
/* Block Matrix Multiply */
for(int i = 0; i < N; i += bSize)
	for(int k = 0; k < N; k += bSize)
		for(int j = 0; j < N; j += bSize)
			for(int it = i; it < i+bSize; it++)
				for(int kt = k; kt < k+bSize; kt++)
					for(int jt = j; jt < j+bSize; jt++)
					{
						C[it][jt] += A[it][kt]*B[kt][jt];
					}
}


void omp_BMM(double ** A, double ** B, double ** C)
{
int i, j, k, it, jt, kt;
/* openMP version of BMM */
#pragma omp parallel shared(A,B,C), private(i,j,k, it, jt, kt)
{
#pragma omp for schedule(static)
for(int i = 0; i < N; i += bSize)
	for(int k = 0; k < N; k += bSize)
		for(int j = 0; j < N; j += bSize)
			for(int it = i; it < i+bSize; it++)
				for(int kt = k; kt < k+bSize; kt++)
					for(int jt = j; jt < j+bSize; jt++)
						C[it][jt] += A[it][kt]*B[kt][jt];
}
}

\end{lstlisting}
\textbf{Results}
\begin{lstlisting}
(With -O3 optimization)
BMM: 1.654067
omp_BMM: 1.621354 
Armadillo: 1.439744
BLAS: 1.430144
Eigen: 0.644094
(With no compiler optimization)
BMM: 7.912810
omp_BMM: 7.815276 
Armadillo: 1.446131
BLAS: 1.428944
Eigen: 24.925716

\end{lstlisting}
\end{center}

\subsection{Tiled Matrix multiplication}
This is an implementation of matrix multiplication written in pure C++, with the tile size carefully chosen so that whole tiles can fit in my computers L1 cache. I have chosen different tile sizes on different computers, and choosing an optimal tile size depends on the particular machine being used. Matrix multiplication can be executed with only three for loops, but tiling is a common trick to improve temporal locality. Since the computer has a fast cache memory closer to the CPU registers than the main memory, what we want to do is load up the cache as full as we can and then use the cached information as many times as possible before bringing new memory to the cache. Tiling achieves this. 

My implementation is fairly straight forward. I allocate some square 2d arrays and then multiply them using the tradition matrix multiplication for loops,except that the three inner for loops work only on a subset of data specified by the outer for loops.

\subsection{Parallelized MM using OpenMP}
OpenMP is the easiest library to use to parallelize C++ code. All one needs to do is include the headerfile omp.h, and add one or two lines of code before a for loop, and the code will run on all of the cores of a multicore processor. Unfortunatly in the above example, little performance increase is seen. This is likely due to "False Sharing", which is what occurs when multiple processors concurrently try to write to the adjacent locations in global memory. Since each processor has its own cache, and memory is brought to the cache in chunks, if processor 1 is working on A[0] and processor 2 is writing to A[1], no parallelization is possible, since processor 1 will write its work to A[0], and send it back to global memory. Processor 2 will then be unable to send its results back to global memory since the value of A[0] which it has does not match the global value. Processor 2 thus checks out the same memory block, redoes its computation, and then sends this value back to memory. The code then can run slower than the non parallelized code, due to the excess memory transfers. This can be alleviated with relatively simple loop transformations, but I haven't spent time on that. I chose to simply present OpenMP as an option. If we were to write a clever looping for the matrix multiplication, there would be no competition from the other libraries, because they all run only serially. This is a major drawback to BLAS, and can be attributed to its having been written a long time ago when multicore architectures were not available. There are some parallel BLAS implementations available online, and even some which work with CUDA, the C-based GPU programming language.

\subsection{BLAS}
The BLAS are a set of old FORTRAN routines that have been highly optimized over time. In addition to being fast, they are also portable and can be run on many processors. One interesting aspect of the BLAS is the fact that they may give seemingly erroneous results if used improperly in a C++ code. This is because FORTRAN uses column major storage of matrices, whereas C++ uses row major storage. You will notice in the sample code that dgemm takes a pointer to a 1d array as an arguement. Depending on how you want to output the results, you should enter the entries of your matrices carefully. If you enter them in a row major fashion, your output will be transposed. If you decide to circumvent this by using the TRANS argument to dgemm, you have to be careful with the values of LDX, which are described here:

The arguments to the mysteriously named dgemm are:
\begin{enumerate}
\item TRANSX - This is whether or not you want the matrix X transposed. Choices are 'N' for No, 'T' for Transpose, and 'C' for Conjugate Transpose.
\item M - This specifies the number of rows in A and C
\item  N - This specifies the number of columns in B and A
\item K - This specifies the number of columns of A and, consequently, the rows of B.
\item LDX - This specifies the leading dimension of matrix X. This depends upon whether or not you use transposition.
\item ALPHA, BETA - These are coefficients which correspond the the operation performed by dgemm(), namely, $C = \alpha A*B + \beta C$. In the sample code, $\beta$ is set to zero so we simply perform a matrix multiplication.
\item A, B, C - These are the matrices we are using.
\end{enumerate}
\subsection{Armadillo}
Armadillo is a widely used linear algebra package which is largely built upon the BLAS and LAPACK, and which offers a convenient MATLAB like syntax to the user. You will notice in the above code that the sprawling lines of code which are needed for the tiled matrix multiplication, and the confusing syntax which is required to implement dgemm boils down to four easy to read lines when the operation is coded using Armadillo. Operators are nicely overloaded in Armadillo, too, such that if one wants to see the contents of matrix A one can use the familiar $cout << A << endl$, and the output is nicely formatted. In addition, matrix multiplication uses $*$, which makes it easy to code (even the super user friendly language Python library numpy requires users to use dot(A,B) to compute  the product A*B).

Since Armadillo is essentially a front end to the BLAS, we hould see very little performance difference between it and the pure BLAS code. Indeed, the overhead generated by Armadillo is quite small, and the run times are about the same for both.

\subsection{Eigen}
The Eigen syntax is not terribly complicated, but is more confusing than the Armadillo syntax. In addition, downloading and linking Eigen is slightly more complicated in that you have to include a header, and provide a symlink to the directory where certain functions are stored when you compile. For the user not thoroughy comfortable with a UNIX environment, this is a serious hassle. The good news is, however, that Eigen provides not only common linear algebra operations, but also comes with a whole suite of overloaded operators which make using the library a real pleasure. These are the reasons why Eigen is used by so many production quality software development teams. The price on pays for the convenience of Eigen is nevertheless quite high. It seems that Eigen requires a great deal of overhead which O3 optimization miraculously gets rid of - compare the run times with and without optimization. While Eigen is easy to use, the fact that the run time with and without optimizatin is so disparate make one wonder what exactly the compiler is throwing away and if it is safe. O3 optimized Eigen code runs much faster than optimized BLAS code, and seeing how Eigen is based on BLAS, one ought to feel some unease with the shortcuts taken by the compiler.
 
\section{Python}
\subsection{Code}
\begin{center}
\textbf{Here is the code we will be discussing.}
\begin{lstlisting}
""" In this script we will see how to compute matrix - matrix products
using numpy and scipy.linalg.BLAS 

From the output we see the python methods are similar.
"""

from numpy import matrix, array, dot, zeros
from numpy.random import random
from scipy.linalg.blas import dgemm
import time

N = 1024
NITER = 30

A_array = random((N,N))
B_array = random((N,N))
C_array = zeros((N,N))

start = time.time()
for it in range(NITER):
	C_array = dot(A_array,B_array)
stop = time.time()

print"Time for 2d array dot product is:", (stop - start)/NITER

A_mat = matrix(A_array)
B_mat = matrix(B_array)
C_mat = matrix(C_array)

start = time.time()
for it in range(NITER):
	C_mat = A_mat*B_mat
stop = time.time()
print "Time for Matrix multiplication is:", (stop-start)/NITER

alpha = 1
start = time.time()
for it in range(NITER):
	C_array = dgemm(alpha, A_array, B_array)
stop = time.time()
print "Time for dgemm is:", (stop-start)/NITER
\end{lstlisting}
\textbf{Results}
\begin{lstlisting}
Time for 2d array dot product is: 0.299135661125
Time for Matrix multiplication is: 0.304835136731
Time for dgemm is: 0.367184797923
\end{lstlisting}
\end{center}
\subsection {A quick comparison}
It is easy to talk about Python code, because it is so succinct. In this code I compare matrix multiplication using numpy 2d arrays, numpy 2d matrices, and the low-level BLAS function dgemm. One right away notices that the array and matrix multiplications run in exactly the same amount of time. This is because numpy matrices are nothing more than numpy arrays with a bit more structure. One can quickly see this since the matrices above are instantiated by copying arrays. 
The Python optimizer seems to deal with the matrices as if they were arrays, stripping away the extra information they contain. The numpy website nevertheless recommends that users stay away from matrices as they are not as heavily tested as the matrices are. In fact, the numpy website offers a quick answer the the question ``Should I use arrays or matrices?" ``Short Answer: Arrays". In  the above code one can see that the matrix multiplication is done witha arrays using the dot() function, and with matrices using the convenient * - while is is much more readable, it should be reiterated that matrices are not as reliable as arrays for more complicated operations. 
You can also see that dgemm is a bit slower than the built in Python functions. This is rather strange due the  fact that the numpy routines are buily upone BLAS and LAPACK. This, just like the Eigen example, needs much further investigation.
The really interesting bit of information is that the \emph{Python code runs much faster than the C++ code} in either case! This is exactly opposite our intuition and dispells the common stereotype that Python is slow and useless for scientific computing. In some cases, the Python optimizer is able to far outdo the g++ compiler! (Unfortunaely there is no way to link Eigen and Armadillo to the icc compiler, so I wan unable to see how fast my code would bu in that case. Typically, ICC far outdoes g++).