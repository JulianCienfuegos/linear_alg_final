\chapter{Conclusion}

In the end we see that Armadillo and the BLAS offer incredible speed ups over the cache efficient code which one can write in C++. This is because for the routines we considered, Armadillo is simply a front end to the BLAS. Should we choose to not use compiler optimization (say, because we don't trust -O3), then the best bet is probably to use Armadillo. This library offers speed and a MATLAB like syntax which is very easy to understand. If, on the other hand, we would like to use a compiler optimizer, then (at least on my machine), Eigen has shown itself to be the clear winner, running in roughly one third of the time of the BLAS. Without optimization, however, Eigen has a great deal of overhead which slows it down considerably. In addition, Eigen has a rather confusing syntax, and is a bit more difficult to both install and compile with. According to many sources, Eigen is often used in market-quality software. But, for the common student, it is probably not worth the expense of learning how to use.

If we turn our attention to Python, there is no contest. The Python motto is "Everything is an object", and this is clear when you try to implement libraries which are not specifically tailored to Python in the language. We see that the numpy routines offer incredible speed ups over the pure BLAS functions. This is because (although at their lowest leverl, the Python routines do often implement the BLAS routines, they do so in a carefully optimized fashion. In a code which I have already shared on my github account I showed how Python matrix multiplication is not sped up, even through the cache-efficiency tricks which work in C. This is due to the fact that Python has a built in optimizer which is very smart about how to handle complicated Python objects.